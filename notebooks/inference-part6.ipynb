{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28204d02-5070-4b89-82ff-b20b5c978d14",
   "metadata": {},
   "source": [
    "# <center>ASTR4004/8004 - Inference - Part 6</center> \n",
    "\n",
    "## Simulation-based inference\n",
    "\n",
    "Our goal is to perform inference on a model's parameters $\\theta$ given observations $D$ and learn the posterior distribution $P(\\theta|D)$. Normally, we do this with Bayes' rule:\n",
    "$$\n",
    "P(\\theta|D)=P(D|\\theta)\\frac{P(\\theta)}{P(D)},\n",
    "$$\n",
    "which relies on the likelihood function $P(D|\\theta)$.\n",
    "\n",
    "<font color='red'> However, what if we don't know the likelihood or there is no functional form to evaluate the likelihood?</font>\n",
    "\n",
    "You can train a classifier $\\tilde{P}(y = 1 \\mid \\theta, D)$ to distinguish whether the sample $(\\theta, D)$ is drawn jointly $(y=1)$ or marginally $(y=0)$. The classifier will inform the likelihood-to-evidence ratio.\n",
    "\n",
    "$$\n",
    "\\frac{P(D|\\theta)}{P(D)} = \\frac{P(\\theta|D)}{P(\\theta)} =  \n",
    "\\frac{P(\\theta, D)}{P(\\theta)P(D)}\n",
    "\\equiv \n",
    "\\frac{\\tilde{P}(\\theta, D \\mid y = 1)}{\\tilde{P}(\\theta, D \\mid y = 0)} \n",
    "= \n",
    "\\frac{\\tilde{P}(\\theta, D, y = 1)}{\\tilde{P}(\\theta, D, y = 0)} \n",
    "= \n",
    "\\frac{\\tilde{P}(y = 1 \\mid \\theta, D)}{\\tilde{P}(y = 0 \\mid \\theta, D)} \n",
    "= \n",
    "\\frac{\\tilde{P}(y = 1 \\mid \\theta, D)}{1 - \\tilde{P}(y = 1 \\mid \\theta, D)}\n",
    "$$\n",
    "\n",
    "This notebook shows a simple example of model recovery using `swyft` and its backend PyTorch (`torch`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62afe66-7673-4489-8a44-6266e67371f1",
   "metadata": {},
   "source": [
    "## Useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ececb83d-60ec-4635-b847-694bfc5c794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import swyft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10de334-f922-4282-8bab-6561cb71bc52",
   "metadata": {},
   "source": [
    "## Load data and prepare the input and output pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230d57a-1a38-4482-b4d5-a550c1f29b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../../data/samples_m_t.dat')\n",
    "np.random.seed(0) # fix your seeds for reproducibility!\n",
    "shuffle_index = np.random.permutation(len(data))\n",
    "data = data[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6601b3-bff0-448f-a868-f82bca8e4662",
   "metadata": {},
   "source": [
    "In this example, the first two columns correspond to dark matter mass in units of eV and lifetime in units of second, while the following are effective parameters characterizing the heating, ionization and excitation coefficients from dark matter to the intergalactic medium. In particular, the 2nd, 3rd, 4th and 5th columns can be used to compute the heating coefficient normalized by the lifetime as a function of redshift following a Schechter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8223bf9-94b4-44e7-acca-c83a4f6de2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify input parameters and output observables (i.e., fheat)\n",
    "params = np.log10(data[:,0]).reshape([-1,1])\n",
    "\n",
    "zs = np.arange(5, 35)\n",
    "fheat = data[:,2]+np.log10(data[:,1]) + np.log10(np.exp(data[:,3]*(zs[:,None]-15)) * ((zs[:,None])/15)**data[:,4])\n",
    "fheat = fheat.T\n",
    "\n",
    "# We keep the first 5 samples as observations (amortization!), and use the rest for training\n",
    "obs = ...\n",
    "samples = ...\n",
    "\n",
    "# let's visualize the dataset\n",
    "for i in range(1000):\n",
    "    plt.plot(zs, samples[i]['fheat'], color='k', lw=0.1)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(zs, obs[i]['fheat'], color='r', lw = 2)\n",
    "    \n",
    "plt.ylabel(r\"$\\log_{10}\\left[f_{\\rm heat}\\right]$\")\n",
    "plt.xlabel(r\"$z$\")\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549af94-9724-4e6c-ac6f-db6630cc13b1",
   "metadata": {},
   "source": [
    "We could also take a look at the joint and marginal samples to get a feeling for the classification that will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1430ee-766b-4b74-bd99-e80658d95473",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "...\n",
    "\n",
    "plt.scatter(..., alpha=0.3, c='r', s=2., label='marginal')\n",
    "plt.scatter(..., alpha=0.3, c='b', s=2., label='joint');\n",
    "plt.xlabel(r'$log_{10}(m_{\\chi}/ {\\rm eV})$')\n",
    "plt.ylabel(r\"$\\log_{10}\\left[f_{\\rm heat}(z=5)\\right]$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd97ea0-e072-4614-a957-e7feaf217758",
   "metadata": {},
   "source": [
    "## Inference network\n",
    "\n",
    "Swyft comes with a few default networks. Here we use swyft.LogRatioEstimator_1dim, which is a dense network that estimates one-dimensional posteriors. You can use LogRatioEstimator_Ndim to estimate higher-dimensional marginalized posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea28a6-cd71-4bb9-a634-3c55a86afe39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Network(swyft.SwyftModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ...\n",
    "        \n",
    "    def forward(self, data, theta):\n",
    "        ...\n",
    "        \n",
    "        return logratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f5577-8928-44f0-9919-97f555189b62",
   "metadata": {},
   "source": [
    "## Training\n",
    "Training is now done using the SwyftTrainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7d983-c7e6-4482-b09b-0cc860cd7019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = swyft.SwyftTrainer(precision = 64)\n",
    "network = Network()\n",
    "model.fit(network, swyft.SwyftDataModule(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4cb9f-b72c-4223-945d-6f6af0631deb",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Since the inference network estimates the logarithm of the posterior-to-prior ratio, we can obtain weighted posterior samples by running many prior samples through the inference network. To this end, we first generate prior samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b77df8-d3b7-4e99-b9c7-4defc760607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logm_min = 6\n",
    "logm_max = 12\n",
    "prior = np.random.rand(100000, 1)\n",
    "prior = prior * (logm_max - logm_min) + logm_min\n",
    "\n",
    "prior_samples = swyft.Samples(logm = prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346afaeb-64c8-456f-9e66-9db71ed9ed29",
   "metadata": {},
   "source": [
    "Then we evaluate the inference network by using the infer method of the swyft.Trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5311f-fb86-496c-8ebc-df69fddb2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    predictions = ...\n",
    "    swyft.plot_posterior(predictions, \"logm[0]\", truth={'logm[0]':obs[i]['logm']}, \n",
    "                    labels = [r'$\\log_{10}(m_{\\chi}/{\\rm eV})$',]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a00a80-d075-43ee-86dd-3bf9b409baa4",
   "metadata": {},
   "source": [
    "## Converage tests\n",
    "\n",
    "How do we know if we are getting it right?\n",
    "\n",
    "If the obtained posterior is accurate, we expect the true parameters to fall outside the p=95% highest posterior density region in only 1-p=5% of the cases. If the chance goes above 5%, the posterior is narrower than it should be and hence overconfident; If the chance is lower than 5%, the posterior is wider than the truth and hence conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1292917-4c53-44c6-962c-e4dc449062bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_samples = model.test_coverage(network, samples[-500:], prior_samples)\n",
    "\n",
    "# The pp figure plots 1-p vs 1-p\n",
    "swyft.plot_pp(coverage_samples, \"logm[0]\")\n",
    "plt.text(0.4, 0.5, \"...\", rotation=45,\n",
    "        color=\"green\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.text(0.45, 0.35, \"...\", rotation=45,\n",
    "        color=\"green\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# The zz figure plots \"sigma\" vs \"sigma\" as z is defined as 1-0.5*(1-p)\n",
    "swyft.plot_zz(coverage_samples, \"logm[0]\")\n",
    "plt.text(2.5, 3, \"...\", rotation=45,\n",
    "        color=\"green\", fontsize=12, ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.text(2.7, 2.3, \"...\", rotation=45,\n",
    "        color=\"green\", fontsize=12, ha=\"center\", va=\"center\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASTR4004_8004",
   "language": "python",
   "name": "astr4004_8004"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
